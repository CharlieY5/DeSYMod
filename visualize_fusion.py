import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
matplotlib.rcParams['axes.unicode_minus'] = False
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, accuracy_score
import os
import numpy as np

st.set_page_config(page_title="Fusion Model Evaluation", layout="wide", page_icon="ğŸ¤–", initial_sidebar_state="expanded")

# æ·±è‰²ç§‘æŠ€é£ä¸»é¢˜
st.markdown("""
    <style>
    body, .stApp {background-color: #181c24; color: #e0e6f1;}
    .css-1d391kg {background-color: #23272f;}
    .css-1v0mbdj {background-color: #23272f;}
    .stButton>button {background-color: #23272f; color: #e0e6f1; border-radius: 8px;}
    .stDataFrame {background-color: #23272f; color: #e0e6f1;}
    </style>
""", unsafe_allow_html=True)

st.title("ğŸ¤– èåˆæ¨¡å‹è¯„æµ‹ä¸å¯è§†åŒ– (Fusion Model Evaluation)")

# é˜ˆå€¼é€‰æ‹©
st.sidebar.subheader("ğŸ”§ é˜ˆå€¼é…ç½®")
threshold = st.sidebar.slider(
    "åˆ†ç±»é˜ˆå€¼", 
    min_value=0.1, 
    max_value=0.9, 
    value=0.4238,  # é»˜è®¤æœ€ä¼˜é˜ˆå€¼
    step=0.01,
    help="è°ƒæ•´åˆ†ç±»é˜ˆå€¼ä»¥å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡"
)

# é¢„è®¾é˜ˆå€¼æŒ‰é’®
col1, col2, col3 = st.sidebar.columns(3)
if col1.button("æœ€ä¼˜F1(0.424)"):
    threshold = 0.4238
if col2.button("å¹³è¡¡(0.542)"):
    threshold = 0.5423
if col3.button("ä¼ ç»Ÿ(0.5)"):
    threshold = 0.5

st.sidebar.write(f"**å½“å‰é˜ˆå€¼**: {threshold:.3f}")

# è¯»å–ç»“æœcsv
csv_path = st.sidebar.text_input("ç»“æœCSVè·¯å¾„", "results/fusion_test_results.csv")
if not os.path.exists(csv_path):
    st.warning(f"æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶: {csv_path}")
    st.stop()

df = pd.read_csv(csv_path)

# ä½¿ç”¨é€‰æ‹©çš„é˜ˆå€¼é‡æ–°è®¡ç®—é¢„æµ‹ç»“æœ
df['pred_adjusted'] = (df['score'] > threshold).astype(int)

# åŠ¨æ€æŒ‡æ ‡å±•ç¤º
col1, col2, col3, col4, col5 = st.columns(5)
acc = (df['label'] == df['pred_adjusted']).mean()
col1.metric("å‡†ç¡®ç‡(Accuracy)", f"{acc:.4f}")
prec = precision_score(df['label'], df['pred_adjusted'])
rec = recall_score(df['label'], df['pred_adjusted'])
f1 = f1_score(df['label'], df['pred_adjusted'])
auc_score = roc_auc_score(df['label'], df['score'])
col2.metric("ç²¾ç¡®ç‡(Precision)", f"{prec:.4f}")
col3.metric("å¬å›ç‡(Recall)", f"{rec:.4f}")
col4.metric("F1åˆ†æ•°", f"{f1:.4f}")
col5.metric("AUC", f"{auc_score:.4f}")

# æ··æ·†çŸ©é˜µ
st.subheader("æ··æ·†çŸ©é˜µ Confusion Matrix")
cm = confusion_matrix(df['label'], df['pred_adjusted'])
fig_cm, ax_cm = plt.subplots(figsize=(4,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax_cm)
ax_cm.set_xlabel('Predicted')
ax_cm.set_ylabel('True')
st.pyplot(fig_cm)

# ROCæ›²çº¿
st.subheader("ROCæ›²çº¿ Receiver Operating Characteristic")
fpr, tpr, _ = roc_curve(df['label'], df['score'])
roc_auc = auc(fpr, tpr)
fig_roc, ax_roc = plt.subplots()
ax_roc.plot(fpr, tpr, color='#00e6e6', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
ax_roc.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
ax_roc.set_xlim([0.0, 1.0])
ax_roc.set_ylim([0.0, 1.05])
ax_roc.set_xlabel('å‡æ­£ç‡ (False Positive Rate)')
ax_roc.set_ylabel('çœŸæ­£ç‡ (True Positive Rate)')
ax_roc.set_title('æ¥æ”¶è€…æ“ä½œç‰¹å¾æ›²çº¿ (Receiver Operating Characteristic)')
ax_roc.legend(loc="lower right")
st.pyplot(fig_roc)

# è¯¦ç»†è¡¨æ ¼
st.subheader("è¯¦ç»†é¢„æµ‹ç»“æœè¡¨æ ¼")
# æ˜¾ç¤ºåŸå§‹é¢„æµ‹å’Œè°ƒæ•´åçš„é¢„æµ‹å¯¹æ¯”
df_display = df.copy()
df_display['åŸå§‹é¢„æµ‹'] = df['pred']
df_display['è°ƒæ•´é¢„æµ‹'] = df['pred_adjusted']
st.dataframe(df_display[['label', 'åŸå§‹é¢„æµ‹', 'è°ƒæ•´é¢„æµ‹', 'score']].head(50), use_container_width=True)

# åŠ¨æ€ç­›é€‰/ä¸‹è½½
st.download_button("ä¸‹è½½è°ƒæ•´åç»“æœCSV", data=df_display.to_csv(index=False), file_name="fusion_test_results_adjusted.csv")

# å±•ç¤ºè®­ç»ƒé›†/æµ‹è¯•é›†æ ·æœ¬æ•°
st.sidebar.markdown("**æ€»æ ·æœ¬æ•°:** " + str(len(df)))
if 'label' in df:
    st.sidebar.markdown("**æ­£æ ·æœ¬æ•°:** " + str(df['label'].sum()))
    st.sidebar.markdown("**è´Ÿæ ·æœ¬æ•°:** " + str(len(df) - df['label'].sum()))

# æ ·æœ¬åˆ†å¸ƒæ¡å½¢å›¾
st.subheader("ç±»åˆ«åˆ†å¸ƒ (Class Distribution)")
if 'label' in df:
    fig_dist, ax_dist = plt.subplots()
    df['label'].value_counts().sort_index().plot(kind='bar', ax=ax_dist, color=['#00e6e6', '#e67e22'])
    ax_dist.set_xticklabels(['è´Ÿæ ·æœ¬ (0)', 'æ­£æ ·æœ¬ (1)'], rotation=0)
    ax_dist.set_ylabel('æ•°é‡ (Count)')
    ax_dist.set_xlabel('ç±»åˆ« (Class)')
    ax_dist.set_title('ç±»åˆ«åˆ†å¸ƒ (Class Distribution)')
    st.pyplot(fig_dist)

# é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾
if 'score' in df:
    st.subheader("Score Distribution")
    fig_score, ax_score = plt.subplots()
    ax_score.hist(df['score'], bins=20, color='#00e6e6', alpha=0.7)
    ax_score.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.3f}')
    ax_score.set_xlabel('Predicted Score')
    ax_score.set_ylabel('Count')
    ax_score.set_title('Score Distribution with Threshold')
    ax_score.legend()
    st.pyplot(fig_score)

# é”™è¯¯æ ·æœ¬è¡¨æ ¼
st.subheader("é”™è¯¯é¢„æµ‹æ ·æœ¬ (Wrong Predicted Samples)")
if 'label' in df and 'pred_adjusted' in df:
    wrong_df = df[df['label'] != df['pred_adjusted']]
    st.write("é”™è¯¯æ ·æœ¬æ•°é‡: " + str(len(wrong_df)))
    st.dataframe(wrong_df.head(20), use_container_width=True)

# ä¸»è¦æŒ‡æ ‡è¶‹åŠ¿æŠ˜çº¿å›¾ï¼ˆå¦‚æœ‰epochåˆ—ï¼‰
if 'epoch' in df.columns:
    st.subheader("ä¸»è¦æŒ‡æ ‡è¶‹åŠ¿ vs. Epoch (Main Metrics Trend vs. Epoch)")
    metric_cols = [col for col in df.columns if col not in ['epoch', 'label', 'pred_adjusted', 'score']]
    for metric in metric_cols:
        fig_metric, ax_metric = plt.subplots()
        ax_metric.plot(df['epoch'], df[metric], marker='o')
        ax_metric.set_xlabel('Epoch')
        ax_metric.set_ylabel(metric)
        ax_metric.set_title(f'{metric} Trend')
        st.pyplot(fig_metric)

# ç½®ä¿¡åº¦æœ€é«˜çš„å‰10ä¸ªé¢„æµ‹
st.subheader("ç½®ä¿¡åº¦æœ€é«˜çš„å‰10ä¸ªé¢„æµ‹ (Top 10 Predictions with Highest Confidence)")
if 'score' in df:
    top10 = df.copy()
    top10['abs_score'] = (top10['score'] - 0.5).abs()
    top10 = top10.sort_values('abs_score', ascending=False).head(10)
    st.dataframe(top10.drop(columns=['abs_score']), use_container_width=True)

# å±•ç¤ºæ­£è´Ÿæ ·æœ¬scoreå‡å€¼/æ–¹å·®
if 'score' in df and 'label' in df:
    pos_scores = df[df['label'] == 1]['score']
    neg_scores = df[df['label'] == 0]['score']
    st.sidebar.markdown("**æ­£æ ·æœ¬åˆ†æ•°å‡å€¼:** " + f"{pos_scores.mean():.4f}")
    st.sidebar.markdown("**æ­£æ ·æœ¬åˆ†æ•°æ–¹å·®:** " + f"{pos_scores.var():.4f}")
    st.sidebar.markdown("**è´Ÿæ ·æœ¬åˆ†æ•°å‡å€¼:** " + f"{neg_scores.mean():.4f}")
    st.sidebar.markdown("**è´Ÿæ ·æœ¬åˆ†æ•°æ–¹å·®:** " + f"{neg_scores.var():.4f}")

# å±•ç¤ºä¸åŒé˜ˆå€¼ä¸‹çš„å‡†ç¡®ç‡/å¬å›ç‡/ç²¾ç¡®ç‡æ›²çº¿
if 'score' in df and 'label' in df:
    st.subheader("å‡†ç¡®ç‡/å¬å›ç‡/ç²¾ç¡®ç‡ vs. é˜ˆå€¼ (Accuracy/Recall/Precision vs. Threshold)")
    thresholds = np.linspace(0, 1, 100)
    accs, precs, recs = [], [], []
    for t in thresholds:
        preds = (df['score'] > t).astype(int)
        accs.append(accuracy_score(df['label'], preds))
        precs.append(precision_score(df['label'], preds, zero_division=0))
        recs.append(recall_score(df['label'], preds, zero_division=0))
    fig_thr, ax_thr = plt.subplots()
    ax_thr.plot(thresholds, accs, label='Accuracy')
    ax_thr.plot(thresholds, precs, label='Precision')
    ax_thr.plot(thresholds, recs, label='Recall')
    ax_thr.axvline(x=threshold, color='red', linestyle='--', label=f'Current: {threshold:.3f}')
    ax_thr.set_xlabel('Threshold')
    ax_thr.set_ylabel('Metric')
    ax_thr.set_title('Metrics vs. Threshold')
    ax_thr.legend()
    st.pyplot(fig_thr)

# PRæ›²çº¿
if 'score' in df and 'label' in df:
    st.subheader("PRæ›²çº¿ (Precision-Recall Curve)")
    precision, recall, _ = precision_recall_curve(df['label'], df['score'])
    fig_pr, ax_pr = plt.subplots()
    ax_pr.plot(recall, precision, color='#e67e22', lw=2)
    ax_pr.set_xlabel('Recall')
    ax_pr.set_ylabel('Precision')
    ax_pr.set_title('Precision-Recall Curve')
    st.pyplot(fig_pr)

# æ··æ·†çŸ©é˜µè¯¦ç»†æ•°å€¼
if 'label' in df and 'pred_adjusted' in df:
    cm = confusion_matrix(df['label'], df['pred_adjusted'])
    st.write('æ··æ·†çŸ©é˜µè¯¦ç»†æ•°å€¼ (Confusion Matrix Detailed Values):')
    st.write(pd.DataFrame(cm, index=['çœŸå®è´Ÿæ ·æœ¬(0)', 'çœŸå®æ­£æ ·æœ¬(1)'], columns=['é¢„æµ‹è´Ÿæ ·æœ¬(0)', 'é¢„æµ‹æ­£æ ·æœ¬(1)']))

# æ­£è´Ÿæ ·æœ¬scoreåˆ†å¸ƒKDEæ›²çº¿
if 'score' in df and 'label' in df:
    st.subheader("æ­£è´Ÿæ ·æœ¬åˆ†æ•°åˆ†å¸ƒKDEæ›²çº¿ (Positive/Negative Samples Score Distribution KDE Curve)")
    fig_kde, ax_kde = plt.subplots()
    try:
        sns.kdeplot(pos_scores, ax=ax_kde, label='æ­£æ ·æœ¬ (Positive)', color='#e67e22')
        sns.kdeplot(neg_scores, ax=ax_kde, label='è´Ÿæ ·æœ¬ (Negative)', color='#00e6e6')
        ax_kde.axvline(x=threshold, color='red', linestyle='--', label=f'é˜ˆå€¼: {threshold:.3f}')
        ax_kde.set_xlabel('åˆ†æ•° (Score)')
        ax_kde.set_ylabel('å¯†åº¦ (Density)')
        ax_kde.legend()
        st.pyplot(fig_kde)
    except Exception as e:
        st.write(f"KDEç»˜å›¾å¤±è´¥: {e}")

# é¢„æµ‹ä¸ºæ­£/è´Ÿçš„æ ·æœ¬æ•°é‡å’Œæ¯”ä¾‹
if 'pred_adjusted' in df:
    st.sidebar.markdown("**é¢„æµ‹æ­£æ ·æœ¬æ•°é‡:** " + str(int((df['pred_adjusted']==1).sum())))
    st.sidebar.markdown("**é¢„æµ‹è´Ÿæ ·æœ¬æ•°é‡:** " + str(int((df['pred_adjusted']==0).sum())))
    st.sidebar.markdown("**é¢„æµ‹æ­£æ ·æœ¬æ¯”ä¾‹:** " + f"{(df['pred_adjusted']==1).mean()*100:.1f}%")
    st.sidebar.markdown("**é¢„æµ‹è´Ÿæ ·æœ¬æ¯”ä¾‹:** " + f"{(df['pred_adjusted']==0).mean()*100:.1f}%") 